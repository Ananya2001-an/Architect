{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Backend Experiment Number 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this experiment is to see if we can combine the chains in experiments one and two into one chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import openai\n",
    "from langchain.agents import Tool\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import asyncio\n",
    "\n",
    "\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from pydantic import Extra\n",
    "\n",
    "from langchain.schema import BaseLanguageModel\n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForChainRun,\n",
    "    CallbackManagerForChainRun,\n",
    ")\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.schema import BasePromptTemplate\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_details=\"\"\"\n",
    "Basically you would put your hackathon idea in a text box\n",
    "and we would run like 3 llm chains in the background\n",
    "One for frontend one for backend, and then feed both chains into the last one which evaluates the two first chains if its feasible or not, and if it isn't, calls the first two chains again\n",
    "The first two will also return design specifications, eg an outline of the frontend and backend\n",
    "maybe another textbox in the frontend for skills/target categories\n",
    "\"\"\"\n",
    "project_technologies=\"\"\"\n",
    "OpenAI API\n",
    "StreamLit\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Backend Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backend_func(\n",
    "        inputs: Dict[str, Any],\n",
    "        llm: BaseLanguageModel,\n",
    "        advanced_llm: BaseLanguageModel,\n",
    "    ) -> str:\n",
    "        # Feature Extraction\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"project_details\", \"project_technologies\"],\n",
    "            template=\"\"\"\n",
    "            Given the following project description and tech stack, identify and elaborate on the key backend features that would be necessary for development. The backend features should not involve any frontend components or styling. The backend features should be described in terms of capabilities.\n",
    "            This project is a hackathon project. Break apart the features into MVP and additional features. The MVP should be the minimum features necessary to have a working prototype.\n",
    "            Project description: {project_details}\n",
    "            Technologies: {project_technologies}\n",
    "            \"\"\"\n",
    "        )\n",
    "        chain = LLMChain(llm=llm, prompt=prompt)\n",
    "        backend_features = chain.run(inputs)\n",
    "        \n",
    "        # Specification Creation\n",
    "        specification_prompt = PromptTemplate(\n",
    "            input_variables=[\"backend_features\", \"project_technologies\"],\n",
    "            template=\"\"\"\n",
    "            Given the extracted backend features and the specified skills/technologies, create a detailed technical specification. \n",
    "            This specification should include the technologies to be used, the architecture, the different routes/endpoints, their inputs and outputs, and any potential hardware and startup costs.\n",
    "            However, they should be split into two categories: MVP and additional features. The MVP should be the minimum features necessary to have a working prototype.\n",
    "            You should ignore the technologies for the frontend and focus on the backend.\n",
    "            Please also mention any other technical considerations.\n",
    "            \n",
    "            Backend Features: {backend_features}\n",
    "            \n",
    "            Project Technologies: {project_technologies}\n",
    "            \"\"\"\n",
    "        )\n",
    "        specification_chain = LLMChain(llm=llm, prompt=specification_prompt)\n",
    "        specification = specification_chain.run({\n",
    "            'backend_features': backend_features,\n",
    "            'project_technologies': inputs['project_technologies']\n",
    "        })\n",
    "        \n",
    "        approval_prompt = PromptTemplate(\n",
    "            input_variables=[\"technical_specification\", \"aspect\", \"group_size\", \"group_experience\"],\n",
    "            template=\"\"\"\n",
    "            Given the developed technical specification, conduct a thorough review of the MVP Features only for any inconsistencies or issues. \n",
    "            Also, evaluate whether the MVP Features, can be realistically completed within the two day hackathon for {group_size} people, considering the complexity and the technology stack required.\n",
    "            \n",
    "            The MVP Features are specifically listed under the heading 'MVP Features'. \n",
    "            Please completely disregard any features or sections listed under 'Additional Features' or any similar headers.\n",
    "            This specification is only for the {aspect} aspect of the project, and should not be evaluated for other aspects.\n",
    "\n",
    "            Answer this question: Can the MVP Features be realistically completed within the two day hackathon for {group_size} people with this skill level: {group_experience}?\n",
    "            Output only a json with keys 'approval' and 'comments'. \n",
    "            If yes, the value of 'approval' should be '1' and the value of 'comments' should be an empty string\n",
    "            If not, the value of 'approval' should be '0' and the value of 'comments' should be a string with the issues and inconsistencies listed.\n",
    "\n",
    "            Technical Specification: {technical_specification}\n",
    "            \n",
    "            Output only a json with keys 'approval' and 'comments'. \n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        approval_chain = LLMChain(llm=advanced_llm, prompt=approval_prompt)\n",
    "        approval = approval_chain.run({\n",
    "            'technical_specification': specification,\n",
    "            'aspect': 'backend',\n",
    "            'group_size': inputs['group_size'],\n",
    "            'group_experience': inputs['group_experience']\n",
    "        })\n",
    "        \n",
    "        approvals_object = json.loads(approval)\n",
    "        \n",
    "        return_obj = {\n",
    "            'approval': approvals_object['approval'],\n",
    "            'comments': approvals_object['comments'],\n",
    "            'idea': inputs['project_details'],\n",
    "            'features': backend_features,\n",
    "            'specifications': specification\n",
    "        }\n",
    "        \n",
    "        return json.dumps(return_obj)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing this custom function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, max_tokens=1000, model=\"gpt-3.5-turbo\")\n",
    "advanced_llm = ChatOpenAI(temperature=0, max_tokens=1000, model=\"gpt-4\")\n",
    "\n",
    "output = backend_func(\n",
    "    inputs = {\n",
    "        'project_details': project_details,\n",
    "        \"project_technologies\": project_technologies,\n",
    "        \"group_size\": 4,\n",
    "        \"group_experience\": \"experienced\"\n",
    "    },\n",
    "    llm=llm,\n",
    "    advanced_llm=advanced_llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'approval': '1',\n",
       " 'comments': '',\n",
       " 'idea': \"\\nBasically you would put your hackathon idea in a text box\\nand we would run like 3 llm chains in the background\\nOne for frontend one for backend, and then feed both chains into the last one which evaluates the two first chains if its feasible or not, and if it isn't, calls the first two chains again\\nThe first two will also return design specifications, eg an outline of the frontend and backend\\nmaybe another textbox in the frontend for skills/target categories\\n\",\n",
       " 'features': \"Key Backend Features:\\n\\n1. Text Processing: The backend needs to process the text input provided by the user in the text box. It should extract relevant information and keywords from the input to determine the hackathon idea.\\n\\n2. Machine Learning Model Integration: The backend should integrate with the OpenAI API to run three language models (llm chains) in the background. It needs to handle the communication with the API, send the necessary data, and receive the generated responses.\\n\\n3. Feasibility Evaluation: The backend should evaluate the feasibility of the frontend and backend ideas generated by the language models. It needs to compare the two sets of design specifications and determine if they are feasible or not. If not, it should trigger the first two chains again to generate new ideas.\\n\\n4. Design Specification Generation: The backend should process the outputs from the language models and extract the design specifications for both the frontend and backend. It needs to parse the generated text and extract relevant information such as UI components, functionality, and technologies required.\\n\\n5. Skill/Target Category Input: The backend should handle the input from the additional text box in the frontend where users can specify their skills or target categories. It needs to process this information and incorporate it into the evaluation process to generate more relevant and tailored ideas.\\n\\nMVP Features:\\n\\n1. Text Processing\\n2. Machine Learning Model Integration\\n3. Feasibility Evaluation\\n4. Design Specification Generation\\n\\nAdditional Features:\\n\\n1. Skill/Target Category Filtering: Enhance the evaluation process by considering the user's skills or target categories. The backend can filter the generated ideas based on these inputs to provide more personalized recommendations.\\n\\n2. Iterative Idea Generation: Instead of just triggering the first two chains again when the feasibility evaluation fails, the backend can implement an iterative process where it generates multiple sets of frontend and backend ideas. This can provide the user with a range of options to choose from.\\n\\n3. User Authentication: Implement user authentication to allow users to save their hackathon ideas, track their progress, and collaborate with others. This feature can enhance the overall user experience and make the platform more engaging.\\n\\n4. Idea Ranking and Sorting: Implement a ranking and sorting mechanism for the generated ideas based on various criteria such as feasibility, complexity, novelty, or user preferences. This can help users prioritize and select the most suitable idea for their hackathon project.\\n\\n5. Collaboration and Communication: Introduce features that enable users to collaborate and communicate with team members or other participants. This can include chat functionality, task assignment, version control, and real-time collaboration on design specifications.\\n\\nBy implementing the MVP features, the backend will provide the necessary functionality to generate hackathon ideas, evaluate their feasibility, and extract design specifications. The additional features can be added to enhance the user experience, provide more personalized recommendations, and facilitate collaboration among participants.\",\n",
       " 'specifications': \"Technical Specification:\\n\\nTechnologies:\\n- Backend Framework: StreamLit\\n- Machine Learning Model Integration: OpenAI API\\n\\nArchitecture:\\nThe backend will be built using StreamLit, a Python framework for building interactive web applications. StreamLit will handle the routing and endpoints for the backend. The backend will integrate with the OpenAI API to run the language models and process the generated responses.\\n\\nRoutes/Endpoints:\\n1. POST /process_text:\\n   - Input: Text input provided by the user in the text box\\n   - Output: Relevant information and keywords extracted from the input\\n\\n2. POST /run_language_models:\\n   - Input: Data required by the OpenAI API to run the language models\\n   - Output: Generated responses from the language models\\n\\n3. POST /evaluate_feasibility:\\n   - Input: Design specifications from the frontend and backend generated by the language models\\n   - Output: Feasibility evaluation result (feasible or not)\\n\\n4. POST /generate_design_specifications:\\n   - Input: Outputs from the language models\\n   - Output: Design specifications for both the frontend and backend (UI components, functionality, technologies required)\\n\\n5. POST /handle_skill_input:\\n   - Input: Skills or target categories specified by the user\\n   - Output: Processed information to be incorporated into the evaluation process\\n\\nMVP:\\nThe MVP features include text processing, machine learning model integration, feasibility evaluation, and design specification generation. These features will be implemented using the specified routes/endpoints.\\n\\nAdditional Features:\\n1. Skill/Target Category Filtering:\\n   - Enhance the evaluation process by considering the user's skills or target categories.\\n   - The backend will filter the generated ideas based on these inputs to provide more personalized recommendations.\\n\\n2. Iterative Idea Generation:\\n   - Instead of triggering the first two chains again when the feasibility evaluation fails, the backend will generate multiple sets of frontend and backend ideas.\\n   - This will provide the user with a range of options to choose from.\\n\\n3. User Authentication:\\n   - Implement user authentication to allow users to save their hackathon ideas, track their progress, and collaborate with others.\\n   - This feature will enhance the overall user experience and make the platform more engaging.\\n\\n4. Idea Ranking and Sorting:\\n   - Implement a ranking and sorting mechanism for the generated ideas based on various criteria such as feasibility, complexity, novelty, or user preferences.\\n   - This will help users prioritize and select the most suitable idea for their hackathon project.\\n\\n5. Collaboration and Communication:\\n   - Introduce features that enable users to collaborate and communicate with team members or other participants.\\n   - This can include chat functionality, task assignment, version control, and real-time collaboration on design specifications.\\n\\nTechnical Considerations:\\n- Hardware and Startup Costs: The backend can be hosted on a cloud platform such as AWS or Google Cloud. The costs will depend on the chosen hosting provider and the expected traffic and usage of the application.\\n- Scalability: The backend should be designed to handle multiple concurrent requests and scale based on the application's usage.\\n- Security: Proper security measures should be implemented, including data encryption, secure communication protocols, and user authentication mechanisms.\\n- Error Handling: The backend should handle errors gracefully and provide informative error messages to the users.\\n- Testing: Unit tests and integration tests should be implemented to ensure the correctness and reliability of the backend functionality.\\n- Documentation: Detailed documentation should be provided for the backend APIs, including the input/output formats, error codes, and usage examples.\"}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Architect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
