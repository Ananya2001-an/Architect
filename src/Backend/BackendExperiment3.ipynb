{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Backend Experiment Number 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this experiment is to see if we can combine the chains in experiments one and two into one chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import openai\n",
    "from langchain.agents import Tool\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import asyncio\n",
    "\n",
    "\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from pydantic import Extra\n",
    "\n",
    "from langchain.schema import BaseLanguageModel\n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForChainRun,\n",
    "    CallbackManagerForChainRun,\n",
    ")\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.schema import BasePromptTemplate\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_details=\"\"\"\n",
    "Basically you would put your hackathon idea in a text box\n",
    "and we would run like 3 llm chains in the background\n",
    "One for frontend one for backend, and then feed both chains into the last one which evaluates the two first chains if its feasible or not, and if it isn't, calls the first two chains again\n",
    "The first two will also return design specifications, eg an outline of the frontend and backend\n",
    "maybe another textbox in the frontend for skills/target categories\n",
    "\"\"\"\n",
    "project_technologies=\"\"\"\n",
    "OpenAI API\n",
    "StreamLit\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Backend Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def backend_func(\n",
    "        inputs: Dict[str, Any],\n",
    "        llm: BaseLanguageModel,\n",
    "        advanced_llm: BaseLanguageModel,\n",
    "    ) -> str:\n",
    "        # Feature Extraction\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"project_details\", \"project_technologies\"],\n",
    "            template=\"\"\"\n",
    "            Given the following project description and tech stack, identify and elaborate on the key backend features that would be necessary for development. The backend features should not involve any frontend components or styling. The backend features should be described in terms of capabilities.\n",
    "            This project is a hackathon project. Break apart the features into MVP and additional features. The MVP should be the minimum features necessary to have a working prototype.\n",
    "            Project description: {project_details}\n",
    "            Technologies: {project_technologies}\n",
    "            \"\"\"\n",
    "        )\n",
    "        chain = LLMChain(llm=llm, prompt=prompt)\n",
    "        backend_features = await chain.arun(inputs)\n",
    "        \n",
    "        # Specification Creation\n",
    "        specification_prompt = PromptTemplate(\n",
    "            input_variables=[\"backend_features\", \"project_technologies\"],\n",
    "            template=\"\"\"\n",
    "            Given the extracted backend features and the specified skills/technologies, create a detailed technical specification. \n",
    "            This specification should include the technologies to be used, the architecture, the different routes/endpoints, their inputs and outputs, and any potential hardware and startup costs.\n",
    "            However, they should be split into two categories: MVP and additional features. The MVP should be the minimum features necessary to have a working prototype.\n",
    "            You should ignore the technologies for the frontend and focus on the backend.\n",
    "            Please also mention any other technical considerations.\n",
    "            \n",
    "            Backend Features: {backend_features}\n",
    "            \n",
    "            Project Technologies: {project_technologies}\n",
    "            \"\"\"\n",
    "        )\n",
    "        specification_chain = LLMChain(llm=llm, prompt=specification_prompt)\n",
    "        specification = await specification_chain.arun({\n",
    "            'backend_features': backend_features,\n",
    "            'project_technologies': inputs['project_technologies']\n",
    "        })\n",
    "        \n",
    "        approval_prompt = PromptTemplate(\n",
    "            input_variables=[\"technical_specification\", \"aspect\", \"group_size\", \"group_experience\"],\n",
    "            template=\"\"\"\n",
    "            Given the developed technical specification, conduct a thorough review of the MVP Features only for any inconsistencies or issues. \n",
    "            Also, evaluate whether the MVP Features, can be realistically completed within the two day hackathon for {group_size} people, considering the complexity and the technology stack required.\n",
    "            \n",
    "            The MVP Features are specifically listed under the heading 'MVP Features'. \n",
    "            Please completely disregard any features or sections listed under 'Additional Features' or any similar headers.\n",
    "            This specification is only for the {aspect} aspect of the project, and should not be evaluated for other aspects.\n",
    "\n",
    "            Answer this question: Can the MVP Features be realistically completed within the two day hackathon for {group_size} people with this skill level: {group_experience}?\n",
    "            Output only a json with keys 'approval' and 'comments'. \n",
    "            If yes, the value of 'approval' should be '1' and the value of 'comments' should be an empty string\n",
    "            If not, the value of 'approval' should be '0' and the value of 'comments' should be a string with the issues and inconsistencies listed.\n",
    "\n",
    "            Technical Specification: {technical_specification}\n",
    "            \n",
    "            Output only a json with keys 'approval' and 'comments'. \n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        approval_chain = LLMChain(llm=advanced_llm, prompt=approval_prompt)\n",
    "        approval = await approval_chain.arun({\n",
    "            'technical_specification': specification,\n",
    "            'aspect': 'backend',\n",
    "            'group_size': inputs['group_size'],\n",
    "            'group_experience': inputs['group_experience']\n",
    "        })\n",
    "        \n",
    "        approvals_object = json.loads(approval)\n",
    "        \n",
    "        return_obj = {\n",
    "            'approval': approvals_object['approval'],\n",
    "            'comments': approvals_object['comments'],\n",
    "            'idea': inputs['project_details'],\n",
    "            'features': backend_features,\n",
    "            'specifications': specification\n",
    "        }\n",
    "        \n",
    "        return json.dumps(return_obj)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing this custom function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, max_tokens=1000, model=\"gpt-3.5-turbo\")\n",
    "advanced_llm = ChatOpenAI(temperature=0, max_tokens=1000, model=\"gpt-4\")\n",
    "\n",
    "output = await backend_func(\n",
    "    inputs = {\n",
    "        'project_details': project_details,\n",
    "        \"project_technologies\": project_technologies,\n",
    "        \"group_size\": 4,\n",
    "        \"group_experience\": \"experienced\"\n",
    "    },\n",
    "    llm=llm,\n",
    "    advanced_llm=advanced_llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'approval': '1',\n",
       " 'comments': '',\n",
       " 'idea': \"\\nBasically you would put your hackathon idea in a text box\\nand we would run like 3 llm chains in the background\\nOne for frontend one for backend, and then feed both chains into the last one which evaluates the two first chains if its feasible or not, and if it isn't, calls the first two chains again\\nThe first two will also return design specifications, eg an outline of the frontend and backend\\nmaybe another textbox in the frontend for skills/target categories\\n\",\n",
       " 'features': 'Key Backend Features:\\n\\n1. Text Processing: The backend should be able to receive and process the text input from the frontend text box. It should handle the input data and extract relevant information for further processing.\\n\\n2. Language Model Chains: The backend should implement three language model chains using the OpenAI API. One chain should be dedicated to the frontend, another to the backend, and the third one to evaluate the feasibility of the project. These chains will generate responses based on the input data and provide design specifications.\\n\\n3. Integration of Language Models: The backend should integrate the outputs of the frontend and backend language model chains into the evaluation chain. It should pass the generated responses from the frontend and backend chains as inputs to the evaluation chain for further analysis.\\n\\n4. Feasibility Evaluation: The backend should evaluate the feasibility of the project by analyzing the outputs of the frontend and backend chains. It should compare the design specifications and determine if they are compatible or not. If the evaluation indicates infeasibility, the backend should trigger the frontend and backend chains again for further iterations.\\n\\n5. Skill/Target Category Input: The backend should handle the input from the frontend text box for skills or target categories. It should process this information and incorporate it into the language model chains for generating more accurate design specifications.\\n\\nMVP Features:\\n\\n1. Text Processing\\n2. Language Model Chains (Frontend, Backend, Evaluation)\\n3. Integration of Language Models\\n4. Feasibility Evaluation\\n\\nAdditional Features:\\n\\n1. Skill/Target Category Input: Implement the functionality to receive and process input related to skills or target categories. This can enhance the accuracy of the generated design specifications by tailoring them to specific requirements.\\n2. Iterative Chain Execution: Enhance the feasibility evaluation by allowing multiple iterations of the frontend and backend chains if the initial design specifications are deemed infeasible. This iterative process can help refine the project idea and generate more compatible design specifications.\\n3. Error Handling: Implement robust error handling mechanisms to handle any issues that may arise during the execution of the language model chains or the feasibility evaluation. This can include error logging, exception handling, and appropriate error messages to the frontend for better user experience.\\n4. Performance Optimization: Optimize the backend processes to ensure efficient execution of the language model chains and feasibility evaluation. This can involve techniques such as caching, parallel processing, and resource management to minimize response times and improve scalability.\\n5. Security Measures: Implement necessary security measures to protect user data and prevent unauthorized access to the backend system. This can include encryption of sensitive information, secure communication protocols, and user authentication mechanisms.',\n",
       " 'specifications': 'Technical Specification:\\n\\nMVP:\\n\\n1. Text Processing:\\n   - Receive and process text input from the frontend text box.\\n   - Extract relevant information from the input data for further processing.\\n\\n2. Language Model Chains:\\n   - Implement three language model chains using the OpenAI API.\\n   - One chain dedicated to the frontend, another to the backend, and the third one for feasibility evaluation.\\n   - Generate responses based on the input data and provide design specifications.\\n\\n3. Integration of Language Models:\\n   - Integrate the outputs of the frontend and backend language model chains into the evaluation chain.\\n   - Pass the generated responses from the frontend and backend chains as inputs to the evaluation chain for further analysis.\\n\\n4. Feasibility Evaluation:\\n   - Evaluate the feasibility of the project by analyzing the outputs of the frontend and backend chains.\\n   - Compare the design specifications and determine compatibility.\\n   - Trigger the frontend and backend chains again for further iterations if infeasibility is indicated.\\n\\nAdditional Features:\\n\\n1. Skill/Target Category Input:\\n   - Implement functionality to receive and process input related to skills or target categories.\\n   - Incorporate this information into the language model chains for generating more accurate design specifications.\\n\\n2. Iterative Chain Execution:\\n   - Enhance the feasibility evaluation by allowing multiple iterations of the frontend and backend chains.\\n   - If the initial design specifications are deemed infeasible, trigger iterative execution to refine the project idea and generate more compatible design specifications.\\n\\n3. Error Handling:\\n   - Implement robust error handling mechanisms to handle issues during the execution of language model chains or feasibility evaluation.\\n   - Include error logging, exception handling, and appropriate error messages to the frontend for better user experience.\\n\\n4. Performance Optimization:\\n   - Optimize backend processes for efficient execution of language model chains and feasibility evaluation.\\n   - Utilize techniques such as caching, parallel processing, and resource management to minimize response times and improve scalability.\\n\\n5. Security Measures:\\n   - Implement necessary security measures to protect user data and prevent unauthorized access to the backend system.\\n   - Include encryption of sensitive information, secure communication protocols, and user authentication mechanisms.\\n\\nTechnologies:\\n\\n- OpenAI API: Utilize the OpenAI API for implementing the language model chains and generating design specifications.\\n- StreamLit: Use StreamLit as the framework for building the backend application and handling the frontend interactions.\\n\\nTechnical Considerations:\\n\\n- Hardware and Startup Costs: The technical specification does not mention any specific hardware requirements or startup costs. However, it is important to consider the computational resources required for executing the language model chains efficiently. This may involve provisioning suitable hardware or utilizing cloud-based services for scalability.\\n- Frontend Technologies: The technical specification focuses solely on the backend and ignores the frontend technologies. It is assumed that the frontend will be developed separately and will interact with the backend through appropriate APIs or communication protocols.\\n- Scalability: Consider the potential scalability requirements of the backend system. Ensure that the architecture and implementation can handle increased user load and data processing demands.\\n- Data Privacy: Implement measures to ensure data privacy and compliance with relevant regulations. This includes handling user data securely, anonymizing sensitive information, and adhering to data protection guidelines.\\n- API Rate Limiting: Take into account the rate limits imposed by the OpenAI API and design the backend system to handle these limitations effectively. Implement appropriate strategies such as caching or request throttling to optimize API usage and prevent exceeding rate limits.\\n- Documentation and Testing: Document the backend system thoroughly, including API specifications, architecture diagrams, and deployment instructions. Conduct comprehensive testing to ensure the reliability and functionality of the system.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Architect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
